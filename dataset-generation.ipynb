{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "90c62e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "c7ca9d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = [\"like\", \"beautiful\", \"love\"]  # , \"beautiful\", \"love\"\n",
    "\n",
    "words = [\n",
    "    {\"word\": \"like\", \"sentiment\": 1},\n",
    "    {\"word\": \"beautiful\", \"sentiment\": 1},\n",
    "    {\"word\": \"love\", \"sentiment\": 1},\n",
    "    {\"word\": \"cool\", \"sentiment\": 1},\n",
    "    {\"word\": \"pretty\", \"sentiment\": 1},\n",
    "    {\"word\": \"nice\", \"sentiment\": 1},\n",
    "    \n",
    "    {\"word\": \"bad\", \"sentiment\": -1},\n",
    "    {\"word\": \"no\", \"sentiment\": -1},\n",
    "    {\"word\": \"shit\", \"sentiment\": -1},\n",
    "    {\"word\": \"fuck\", \"sentiment\": -1},\n",
    "    {\"word\": \"nasty\", \"sentiment\": -1},\n",
    "    {\"word\": \"hell\", \"sentiment\": -1}\n",
    "]\n",
    "\n",
    "# {\"word\": \"perfect\", \"sentiment\": 1},\n",
    "#     {\"word\": \"excellent\", \"sentiment\": 1},\n",
    "#     {\"word\": \"great\", \"sentiment\": 1},\n",
    "\n",
    "dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "14e32932",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word_dict in words:\n",
    "    \n",
    "    word = word_dict[\"word\"]\n",
    "    sentiment = word_dict[\"sentiment\"]\n",
    "    \n",
    "    for num in range(10000):\n",
    "    \n",
    "        new_word = \"\"\n",
    "        \n",
    "        repeat_letter_index = random.randint(0, len(word)-1)\n",
    "\n",
    "        repeat_letter = word[repeat_letter_index]\n",
    "\n",
    "        repeat_count = random.randint(2, 15)\n",
    "\n",
    "        for x in range(0, len(word)):\n",
    "            if(x == repeat_letter_index):\n",
    "                new_word += repeat_letter * repeat_count\n",
    "            else:\n",
    "                new_word += word[x]\n",
    "\n",
    "        dataset.append({\"emphasized\": new_word, \"actual\": word, \"sentiment\": sentiment})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "46841493",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "b00bd2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data_v8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e443c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "026f5c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "        \n",
    "# for word_dict in words:\n",
    "    \n",
    "#     word = word_dict[\"word\"]\n",
    "    \n",
    "#     for num in range(5):\n",
    "    \n",
    "#         new_word = \"\"\n",
    "        \n",
    "#         for n in range(5):\n",
    "            \n",
    "            \n",
    "#             repeat_letter_index = random.randint(0, len(word)-1)\n",
    "\n",
    "#             repeat_letter = word[repeat_letter_index]\n",
    "\n",
    "#             repeat_count = random.randint(2, 15)\n",
    "\n",
    "#             for x in range(0, len(word)):\n",
    "#                 if(x == repeat_letter_index):\n",
    "#                     new_word += repeat_letter * repeat_count\n",
    "#                 else:\n",
    "#                     new_word += word[x]\n",
    "\n",
    "#             dataset.append({\"emphasized\": new_word, \"actual\": word, \"sentiment\": sentiment})\n",
    "\n",
    "#     for num in range(2, 50):\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "#     for word_item in word_dict.items():    \n",
    "        \n",
    "    \n",
    "# for word in words:\n",
    "    \n",
    "#     for num in range(2, 50):\n",
    "        \n",
    "#         new_word = \"\"\n",
    "\n",
    "#         repeat_letter_index = random.randint(0, len(word)-1)\n",
    "\n",
    "#         repeat_letter = word[repeat_letter_index]\n",
    "\n",
    "#         repeat_count = random.randint(2, 15)\n",
    "\n",
    "#         for x in range(0, len(word)):\n",
    "#             if(x == repeat_letter_index):\n",
    "#                 new_word += repeat_letter * repeat_count\n",
    "#             else:\n",
    "#                 new_word += word[x] \n",
    "\n",
    "#         dataset.append({\"emphasized\": new_word, actual: word, sentiment : })\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     print(random.choices(word))\n",
    "\n",
    "#     for letter in word:\n",
    "        \n",
    "#         print(letter)\n",
    "#     print(len(word))\n",
    "    \n",
    "# for x in range(6):\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "c691859e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'emphasized': 'likeeeeeeee', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'likeeee', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'lllllllike', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'likeeeeeeeeeeeeee', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'lllllllike', 'actual': 'like', 'sentiment': 1}]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "87167ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'emphasized': 'liiiiiiiiiiiiiike', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'liike', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'lllllike', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'likee', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'likkkkkkkkkkkkkke', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'likeeeeeeeeeeee', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'llllllllllllllike', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'llllllllike', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'liiike', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'likeeeeeeee', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'likeeeee', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'llllllllike', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'liiiiiiiiiiike', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'llllllllllike', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'likeeeeeeee', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'likkkkkkke', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'likeeee', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'lllllllike', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'likkkke', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'likkkke', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'liiiiiiiiiiiike', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'likeeeee', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'likkkkkkkkkkkkkkke', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'likeeeee', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'liiiiiike', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'liiiiiiiike', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'lllllllike', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'likeeeeeeee', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'likkke', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'likkkke', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'likeeeeeeee', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'likeeeeeeeeeee', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'liiiiiiiiiiiiiiike', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'liiiiiiiiiike', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'liiiiiiiiiiiike', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'liiiiiiiiiiiiike', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'lllllllllllllike', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'liiiiiiiike', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'likeeeeeee', 'actual': 'like', 'sentiment': 1},\n",
       " {'emphasized': 'likkkkkkkkkkkkke', 'actual': 'like', 'sentiment': 1}]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a8a384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e645f34e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
